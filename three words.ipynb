{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the dating score from three-word description"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('processed_data hand-corrected.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>hyperlink</th>\n",
       "      <th>date</th>\n",
       "      <th>A_name</th>\n",
       "      <th>A_gender</th>\n",
       "      <th>B_name</th>\n",
       "      <th>B_gender</th>\n",
       "      <th>A_what_were_you_hoping_for</th>\n",
       "      <th>A_first_impressions</th>\n",
       "      <th>A_what_did_you_talk_about</th>\n",
       "      <th>...</th>\n",
       "      <th>B_marks_out_of_10</th>\n",
       "      <th>B_would_you_meet_again</th>\n",
       "      <th>B_meet_again_bool</th>\n",
       "      <th>A_any_awkward_moments</th>\n",
       "      <th>B_best_thing_about_X</th>\n",
       "      <th>B_any_awkward_moments</th>\n",
       "      <th>A_and_did_you_kiss</th>\n",
       "      <th>B_and_did_you_kiss</th>\n",
       "      <th>A_marks_out_of_10_float</th>\n",
       "      <th>B_marks_out_of_10_float</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.theguardian.com/lifeandstyle/2022/dec/03/blind-date-olivia-shawn</td>\n",
       "      <td>https://www.theguardian.com/lifeandstyle/2022/dec/03/blind-date-olivia-shawn</td>\n",
       "      <td>2022-12-03</td>\n",
       "      <td>olivia</td>\n",
       "      <td>f</td>\n",
       "      <td>shawn</td>\n",
       "      <td>m</td>\n",
       "      <td>A nice evening with good conversation, or at least a good story.</td>\n",
       "      <td>Tall, warm and easy to talk to. He was late, but in his defence, I had said I was going to be late and then I wasn’t.</td>\n",
       "      <td>Sorting recycling at Glastonbury. The moral dilemmas of our respective jobs. Dogs. Reasons to avoid the sea. The value of good storytelling.</td>\n",
       "      <td>...</td>\n",
       "      <td>A solid 8. She’s a sweetheart.</td>\n",
       "      <td>It would be fun to hang out next time I’m in the Big Smoke.</td>\n",
       "      <td>y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.9</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.theguardian.com/lifeandstyle/2022/nov/26/blind-date-calum-ciaran</td>\n",
       "      <td>https://www.theguardian.com/lifeandstyle/2022/nov/26/blind-date-calum-ciaran</td>\n",
       "      <td>2022-11-26</td>\n",
       "      <td>calum</td>\n",
       "      <td>m</td>\n",
       "      <td>ciaran</td>\n",
       "      <td>m</td>\n",
       "      <td>A rich, handsome future husband whose salary I could retire on at 30 and devote myself to fun stuff. More realistically, a fun evening with some nice company.</td>\n",
       "      <td>Ciarán was lovely! I liked his smile. We hugged spontaneously, like old pals.</td>\n",
       "      <td>Our trips to Colombia. Using Scottish/Irish slang in England. Our love of This Is My House. Our mutual distrust of anyone who eats green bananas.</td>\n",
       "      <td>...</td>\n",
       "      <td>9, couldn’t fault him.</td>\n",
       "      <td>I didn’t feel a romantic spark, but we agreed over text afterwards to be pals.</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.theguardian.com/lifeandstyle/2022/nov/19/blind-date-andrew-kath</td>\n",
       "      <td>https://www.theguardian.com/lifeandstyle/2022/nov/19/blind-date-andrew-kath</td>\n",
       "      <td>2022-11-19</td>\n",
       "      <td>andrew</td>\n",
       "      <td>f</td>\n",
       "      <td>kath</td>\n",
       "      <td>f</td>\n",
       "      <td>To meet someone new. I am open to retiring anywhere so met Kath in Birmingham, halfway between her home and mine. I’m willing to travel to meet someone special.</td>\n",
       "      <td>Enthusiasm. We talked easily and happily.</td>\n",
       "      <td>Our children. Food. Animal welfare. Green issues. Brexit. Electric cars. Natural disasters. Faith. The Proms. The sudden death of her husband, and my divorce. Empty nest syndrome.</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>We swapped numbers.</td>\n",
       "      <td>y</td>\n",
       "      <td>I may have asked too many questions but Andrew was always very polite and answered them.</td>\n",
       "      <td>He is a very sensitive gentleman who cares deeply about his family, a value I respect and share. He also takes pleasure in the simple things in life, as do I.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.theguardian.com/lifeandstyle/2022/nov/12/blind-date-joseph-cole</td>\n",
       "      <td>https://www.theguardian.com/lifeandstyle/2022/nov/12/blind-date-joseph-cole</td>\n",
       "      <td>2022-11-12</td>\n",
       "      <td>joseph</td>\n",
       "      <td>m</td>\n",
       "      <td>cole</td>\n",
       "      <td>m</td>\n",
       "      <td>To meet someone new and have a nice meal.</td>\n",
       "      <td>He was cute – great hair – and better than I was expecting. I got lost so was a bit late and he was already there.</td>\n",
       "      <td>Differences between the UK and the US. Politics. Pubs. Favourite movies (Toy Story 2). University … He was really easy to talk to.</td>\n",
       "      <td>...</td>\n",
       "      <td>A well-earned 7.</td>\n",
       "      <td>I would.</td>\n",
       "      <td>y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>He’s unafraid to talk about what he’s passionate about.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.theguardian.com/lifeandstyle/2022/nov/05/blind-date-dick-nicole</td>\n",
       "      <td>https://www.theguardian.com/lifeandstyle/2022/nov/05/blind-date-dick-nicole</td>\n",
       "      <td>2022-11-05</td>\n",
       "      <td>dick</td>\n",
       "      <td>m</td>\n",
       "      <td>nicole</td>\n",
       "      <td>f</td>\n",
       "      <td>To meet a new friend and have a great meal.</td>\n",
       "      <td>Open, vivacious, lively and very friendly.</td>\n",
       "      <td>Our families. Our mutual interest in sketching, drawing and painting – we even showed each other our sketchbooks. Nicole’s were full of lovely studies of people.</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unlikely. There was no flirting. A compliment or two would’ve been nice! He was charmingly flirtatious with the ladies on the table behind at one point – he needs to bring that twinkle to his date. I also think he would prefer that I live locally.</td>\n",
       "      <td>n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unsurprisingly for an architect he draws really well, and I suspect his watercolours are charming. He draws really well – I was delighted to find we both carry a sketchbook. He was complimentary about my efforts.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                             url  \\\n",
       "0  https://www.theguardian.com/lifeandstyle/2022/dec/03/blind-date-olivia-shawn    \n",
       "1   https://www.theguardian.com/lifeandstyle/2022/nov/26/blind-date-calum-ciaran   \n",
       "2    https://www.theguardian.com/lifeandstyle/2022/nov/19/blind-date-andrew-kath   \n",
       "3    https://www.theguardian.com/lifeandstyle/2022/nov/12/blind-date-joseph-cole   \n",
       "4    https://www.theguardian.com/lifeandstyle/2022/nov/05/blind-date-dick-nicole   \n",
       "\n",
       "                                                                       hyperlink  \\\n",
       "0  https://www.theguardian.com/lifeandstyle/2022/dec/03/blind-date-olivia-shawn    \n",
       "1   https://www.theguardian.com/lifeandstyle/2022/nov/26/blind-date-calum-ciaran   \n",
       "2    https://www.theguardian.com/lifeandstyle/2022/nov/19/blind-date-andrew-kath   \n",
       "3    https://www.theguardian.com/lifeandstyle/2022/nov/12/blind-date-joseph-cole   \n",
       "4    https://www.theguardian.com/lifeandstyle/2022/nov/05/blind-date-dick-nicole   \n",
       "\n",
       "        date  A_name A_gender  B_name B_gender  \\\n",
       "0 2022-12-03  olivia        f   shawn        m   \n",
       "1 2022-11-26   calum        m  ciaran        m   \n",
       "2 2022-11-19  andrew        f    kath        f   \n",
       "3 2022-11-12  joseph        m    cole        m   \n",
       "4 2022-11-05    dick        m  nicole        f   \n",
       "\n",
       "                                                                                                                                         A_what_were_you_hoping_for  \\\n",
       "0                                                                                                  A nice evening with good conversation, or at least a good story.   \n",
       "1    A rich, handsome future husband whose salary I could retire on at 30 and devote myself to fun stuff. More realistically, a fun evening with some nice company.   \n",
       "2  To meet someone new. I am open to retiring anywhere so met Kath in Birmingham, halfway between her home and mine. I’m willing to travel to meet someone special.   \n",
       "3                                                                                                                         To meet someone new and have a nice meal.   \n",
       "4                                                                                                                       To meet a new friend and have a great meal.   \n",
       "\n",
       "                                                                                                     A_first_impressions  \\\n",
       "0  Tall, warm and easy to talk to. He was late, but in his defence, I had said I was going to be late and then I wasn’t.   \n",
       "1                                          Ciarán was lovely! I liked his smile. We hugged spontaneously, like old pals.   \n",
       "2                                                                              Enthusiasm. We talked easily and happily.   \n",
       "3     He was cute – great hair – and better than I was expecting. I got lost so was a bit late and he was already there.   \n",
       "4                                                                             Open, vivacious, lively and very friendly.   \n",
       "\n",
       "                                                                                                                                                              A_what_did_you_talk_about  \\\n",
       "0                                          Sorting recycling at Glastonbury. The moral dilemmas of our respective jobs. Dogs. Reasons to avoid the sea. The value of good storytelling.   \n",
       "1                                     Our trips to Colombia. Using Scottish/Irish slang in England. Our love of This Is My House. Our mutual distrust of anyone who eats green bananas.   \n",
       "2  Our children. Food. Animal welfare. Green issues. Brexit. Electric cars. Natural disasters. Faith. The Proms. The sudden death of her husband, and my divorce. Empty nest syndrome.    \n",
       "3                                                    Differences between the UK and the US. Politics. Pubs. Favourite movies (Toy Story 2). University … He was really easy to talk to.   \n",
       "4                    Our families. Our mutual interest in sketching, drawing and painting – we even showed each other our sketchbooks. Nicole’s were full of lovely studies of people.    \n",
       "\n",
       "   ...               B_marks_out_of_10  \\\n",
       "0  ...  A solid 8. She’s a sweetheart.   \n",
       "1  ...          9, couldn’t fault him.   \n",
       "2  ...                               8   \n",
       "3  ...                A well-earned 7.   \n",
       "4  ...                             NaN   \n",
       "\n",
       "                                                                                                                                                                                                                                    B_would_you_meet_again  \\\n",
       "0                                                                                                                                                                                              It would be fun to hang out next time I’m in the Big Smoke.   \n",
       "1                                                                                                                                                                           I didn’t feel a romantic spark, but we agreed over text afterwards to be pals.   \n",
       "2                                                                                                                                                                                                                                      We swapped numbers.   \n",
       "3                                                                                                                                                                                                                                                 I would.   \n",
       "4  Unlikely. There was no flirting. A compliment or two would’ve been nice! He was charmingly flirtatious with the ladies on the table behind at one point – he needs to bring that twinkle to his date. I also think he would prefer that I live locally.   \n",
       "\n",
       "  B_meet_again_bool  \\\n",
       "0                 y   \n",
       "1                 f   \n",
       "2                 y   \n",
       "3                 y   \n",
       "4                 n   \n",
       "\n",
       "                                                                      A_any_awkward_moments  \\\n",
       "0                                                                                       NaN   \n",
       "1                                                                                       NaN   \n",
       "2  I may have asked too many questions but Andrew was always very polite and answered them.   \n",
       "3                                                                                       NaN   \n",
       "4                                                                                       NaN   \n",
       "\n",
       "                                                                                                                                                                                                   B_best_thing_about_X  \\\n",
       "0                                                                                                                                                                                                                   NaN   \n",
       "1                                                                                                                                                                                                                   NaN   \n",
       "2                                                       He is a very sensitive gentleman who cares deeply about his family, a value I respect and share. He also takes pleasure in the simple things in life, as do I.    \n",
       "3                                                                                                                                                               He’s unafraid to talk about what he’s passionate about.   \n",
       "4  Unsurprisingly for an architect he draws really well, and I suspect his watercolours are charming. He draws really well – I was delighted to find we both carry a sketchbook. He was complimentary about my efforts.   \n",
       "\n",
       "  B_any_awkward_moments A_and_did_you_kiss B_and_did_you_kiss  \\\n",
       "0                   NaN                NaN                NaN   \n",
       "1                   NaN                NaN                NaN   \n",
       "2                   NaN                NaN                NaN   \n",
       "3                   NaN                NaN                NaN   \n",
       "4                   NaN                NaN                NaN   \n",
       "\n",
       "  A_marks_out_of_10_float B_marks_out_of_10_float  \n",
       "0                     7.9                     8.0  \n",
       "1                     8.0                     9.0  \n",
       "2                     8.0                     8.0  \n",
       "3                     8.0                     7.0  \n",
       "4                     NaN                     NaN  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good-humoured, creative, curious.</td>\n",
       "      <td>7.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bubbly, fun, intelligent.</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Enthusiastic, energetic, musical.</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Smart, American, sweet.</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lively, open and vivacious.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714</th>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1438 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  text  score\n",
       "0    Good-humoured, creative, curious.    7.9\n",
       "1            Bubbly, fun, intelligent.    8.0\n",
       "2    Enthusiastic, energetic, musical.    8.0\n",
       "3              Smart, American, sweet.    8.0\n",
       "4          Lively, open and vivacious.    NaN\n",
       "..                                 ...    ...\n",
       "714                                NaN    7.0\n",
       "715                                NaN    7.0\n",
       "716                                NaN    9.0\n",
       "717                                NaN    8.0\n",
       "718                                NaN    9.0\n",
       "\n",
       "[1438 rows x 2 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_a = df.rename(columns={'A_describe_X_in_three_words': 'text', 'A_marks_out_of_10_float': 'score'})[['text', 'score']]\n",
    "df_b = df.rename(columns={'B_describe_X_in_three_words': 'text', 'B_marks_out_of_10_float': 'score'})[['text', 'score']]\n",
    "new_df = pd.concat([df_a, df_b], axis=0)\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good-humoured, creative, curious.</td>\n",
       "      <td>7.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bubbly, fun, intelligent.</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Enthusiastic, energetic, musical.</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Smart, American, sweet.</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We discussed this during the date: energetic (not chaotic), fun, honest.</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>Bright, bubbly and bonkers (in a good way).</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>Erudite, stunning, adventurous.</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>Kind, funny, intelligent.</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>Polite, engaging, carnivore.</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>Lively, happy, fun.</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>822 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                         text  \\\n",
       "0                                           Good-humoured, creative, curious.   \n",
       "1                                                   Bubbly, fun, intelligent.   \n",
       "2                                           Enthusiastic, energetic, musical.   \n",
       "3                                                     Smart, American, sweet.   \n",
       "4    We discussed this during the date: energetic (not chaotic), fun, honest.   \n",
       "..                                                                        ...   \n",
       "817                               Bright, bubbly and bonkers (in a good way).   \n",
       "818                                           Erudite, stunning, adventurous.   \n",
       "819                                                 Kind, funny, intelligent.   \n",
       "820                                              Polite, engaging, carnivore.   \n",
       "821                                                       Lively, happy, fun.   \n",
       "\n",
       "     score  \n",
       "0      7.9  \n",
       "1      8.0  \n",
       "2      8.0  \n",
       "3      8.0  \n",
       "4      8.0  \n",
       "..     ...  \n",
       "817    7.0  \n",
       "818   10.0  \n",
       "819    7.0  \n",
       "820    7.0  \n",
       "821    7.0  \n",
       "\n",
       "[822 rows x 2 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = new_df.dropna().reset_index(drop=True)\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     0\n",
       "score    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>bool</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good-humoured, creative, curious.</td>\n",
       "      <td>7.9</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bubbly, fun, intelligent.</td>\n",
       "      <td>8.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Enthusiastic, energetic, musical.</td>\n",
       "      <td>8.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Smart, American, sweet.</td>\n",
       "      <td>8.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We discussed this during the date: energetic (not chaotic), fun, honest.</td>\n",
       "      <td>8.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>Bright, bubbly and bonkers (in a good way).</td>\n",
       "      <td>7.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>Erudite, stunning, adventurous.</td>\n",
       "      <td>10.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>Kind, funny, intelligent.</td>\n",
       "      <td>7.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>Polite, engaging, carnivore.</td>\n",
       "      <td>7.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>Lively, happy, fun.</td>\n",
       "      <td>7.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>822 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                         text  \\\n",
       "0                                           Good-humoured, creative, curious.   \n",
       "1                                                   Bubbly, fun, intelligent.   \n",
       "2                                           Enthusiastic, energetic, musical.   \n",
       "3                                                     Smart, American, sweet.   \n",
       "4    We discussed this during the date: energetic (not chaotic), fun, honest.   \n",
       "..                                                                        ...   \n",
       "817                               Bright, bubbly and bonkers (in a good way).   \n",
       "818                                           Erudite, stunning, adventurous.   \n",
       "819                                                 Kind, funny, intelligent.   \n",
       "820                                              Polite, engaging, carnivore.   \n",
       "821                                                       Lively, happy, fun.   \n",
       "\n",
       "     score   bool  \n",
       "0      7.9  False  \n",
       "1      8.0   True  \n",
       "2      8.0   True  \n",
       "3      8.0   True  \n",
       "4      8.0   True  \n",
       "..     ...    ...  \n",
       "817    7.0  False  \n",
       "818   10.0   True  \n",
       "819    7.0  False  \n",
       "820    7.0  False  \n",
       "821    7.0  False  \n",
       "\n",
       "[822 rows x 3 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.loc[:, 'bool'] = new_df.loc[:, 'score'] >= 8\n",
    "new_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## from-scratch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([822])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import tensor\n",
    "\n",
    "y = tensor(new_df['bool'].values).float()\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([822, 597])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = tensor(vectorizer.fit_transform(new_df['text']).toarray()).float()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['creative' 'curious' 'good' 'humoured']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "text     Good-humoured, creative, curious.\n",
       "score                                  7.9\n",
       "bool                                 False\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names_out()[X[0].numpy().nonzero()])\n",
    "new_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.3134e-01],\n",
       "        [-3.4916e-01],\n",
       "        [-6.3377e-02],\n",
       "        [-3.2861e-01],\n",
       "        [-3.7507e-01],\n",
       "        [-3.7051e-02],\n",
       "        [ 5.3190e-02],\n",
       "        [-2.2156e-01],\n",
       "        [ 4.8554e-01],\n",
       "        [ 2.1481e-01],\n",
       "        [ 1.4890e-01],\n",
       "        [-2.6573e-03],\n",
       "        [ 1.2481e-01],\n",
       "        [-1.6249e-01],\n",
       "        [ 2.9113e-01],\n",
       "        [ 4.2867e-01],\n",
       "        [ 1.1080e-01],\n",
       "        [ 4.2263e-01],\n",
       "        [ 4.5667e-01],\n",
       "        [ 2.9341e-01],\n",
       "        [-2.1037e-01],\n",
       "        [ 1.2086e-02],\n",
       "        [ 2.9181e-01],\n",
       "        [ 3.9918e-01],\n",
       "        [ 1.1695e-01],\n",
       "        [ 4.1202e-02],\n",
       "        [-1.6139e-01],\n",
       "        [ 2.9334e-01],\n",
       "        [-9.7253e-02],\n",
       "        [ 1.6498e-01],\n",
       "        [ 3.2328e-01],\n",
       "        [-3.5465e-01],\n",
       "        [-1.3714e-01],\n",
       "        [ 3.9398e-01],\n",
       "        [ 1.7668e-01],\n",
       "        [ 1.6297e-01],\n",
       "        [ 2.4948e-01],\n",
       "        [ 3.2014e-01],\n",
       "        [-2.3448e-01],\n",
       "        [-3.7022e-01],\n",
       "        [-1.0971e-01],\n",
       "        [-1.8980e-01],\n",
       "        [-2.5936e-01],\n",
       "        [-4.2981e-01],\n",
       "        [-3.1515e-01],\n",
       "        [-2.1585e-01],\n",
       "        [-4.1494e-01],\n",
       "        [ 1.0682e-01],\n",
       "        [ 3.9872e-01],\n",
       "        [-4.2719e-01],\n",
       "        [ 2.1035e-01],\n",
       "        [ 5.7632e-02],\n",
       "        [ 1.2037e-01],\n",
       "        [ 1.8328e-01],\n",
       "        [-1.7347e-01],\n",
       "        [-7.1973e-02],\n",
       "        [ 4.8764e-01],\n",
       "        [-3.9991e-02],\n",
       "        [-1.8529e-01],\n",
       "        [ 2.1496e-01],\n",
       "        [-2.3487e-01],\n",
       "        [-6.4928e-02],\n",
       "        [-3.4023e-01],\n",
       "        [ 1.3430e-01],\n",
       "        [-3.2668e-01],\n",
       "        [-2.7963e-01],\n",
       "        [ 1.4282e-01],\n",
       "        [-3.6348e-01],\n",
       "        [-1.0300e-01],\n",
       "        [ 2.3114e-01],\n",
       "        [-1.7237e-01],\n",
       "        [ 1.0479e-01],\n",
       "        [ 3.3944e-01],\n",
       "        [ 2.0634e-01],\n",
       "        [ 1.6561e-02],\n",
       "        [-9.9879e-02],\n",
       "        [ 4.5535e-03],\n",
       "        [-3.2073e-01],\n",
       "        [-1.8651e-01],\n",
       "        [ 8.1240e-03],\n",
       "        [-1.7981e-01],\n",
       "        [-3.0796e-01],\n",
       "        [ 4.0260e-02],\n",
       "        [ 1.7812e-01],\n",
       "        [-2.8597e-01],\n",
       "        [-4.2142e-01],\n",
       "        [ 4.3563e-01],\n",
       "        [ 6.1349e-02],\n",
       "        [ 5.6126e-02],\n",
       "        [-3.9153e-01],\n",
       "        [ 2.7311e-01],\n",
       "        [ 3.4558e-01],\n",
       "        [ 3.5792e-01],\n",
       "        [-3.6673e-01],\n",
       "        [-5.7294e-02],\n",
       "        [-4.8078e-01],\n",
       "        [ 6.5913e-02],\n",
       "        [-2.1886e-01],\n",
       "        [-6.4128e-02],\n",
       "        [ 4.0806e-01],\n",
       "        [ 3.3308e-01],\n",
       "        [-4.7788e-02],\n",
       "        [ 2.8466e-01],\n",
       "        [-1.2929e-01],\n",
       "        [-1.1405e-01],\n",
       "        [-3.1085e-01],\n",
       "        [ 2.8740e-01],\n",
       "        [ 2.9039e-01],\n",
       "        [ 2.3414e-01],\n",
       "        [-1.7350e-01],\n",
       "        [-2.7719e-01],\n",
       "        [ 3.4749e-01],\n",
       "        [-3.1932e-01],\n",
       "        [ 9.8807e-02],\n",
       "        [ 1.1415e-01],\n",
       "        [ 1.7088e-01],\n",
       "        [ 4.4246e-01],\n",
       "        [-1.2242e-01],\n",
       "        [-1.8856e-01],\n",
       "        [ 3.2769e-01],\n",
       "        [-8.6838e-04],\n",
       "        [-3.3312e-01],\n",
       "        [ 1.3866e-01],\n",
       "        [-2.8134e-01],\n",
       "        [-1.7556e-01],\n",
       "        [-1.7141e-01],\n",
       "        [ 1.7648e-01],\n",
       "        [ 7.0683e-02],\n",
       "        [-2.5303e-01],\n",
       "        [-1.8275e-01],\n",
       "        [ 4.3860e-01],\n",
       "        [-3.4293e-01],\n",
       "        [-1.4781e-01],\n",
       "        [ 2.1224e-01],\n",
       "        [-3.5109e-01],\n",
       "        [ 2.2588e-01],\n",
       "        [-1.2774e-02],\n",
       "        [-3.1025e-01],\n",
       "        [-4.4369e-01],\n",
       "        [ 2.4050e-01],\n",
       "        [-3.1901e-01],\n",
       "        [-4.7179e-01],\n",
       "        [ 2.3708e-01],\n",
       "        [ 1.3911e-01],\n",
       "        [-1.8700e-01],\n",
       "        [-2.6670e-01],\n",
       "        [ 3.3116e-01],\n",
       "        [-4.0228e-01],\n",
       "        [ 9.7921e-02],\n",
       "        [-3.0189e-01],\n",
       "        [ 3.5088e-01],\n",
       "        [ 4.4981e-01],\n",
       "        [-9.0528e-02],\n",
       "        [-1.2809e-01],\n",
       "        [-2.1987e-03],\n",
       "        [ 3.3317e-01],\n",
       "        [ 3.6832e-01],\n",
       "        [ 4.6925e-01],\n",
       "        [-2.8053e-01],\n",
       "        [ 9.9216e-02],\n",
       "        [-2.6670e-01],\n",
       "        [ 3.0382e-01],\n",
       "        [ 3.9798e-01],\n",
       "        [ 1.1296e-01],\n",
       "        [-1.8715e-01],\n",
       "        [-2.1483e-01],\n",
       "        [ 4.2094e-01],\n",
       "        [ 1.8913e-02],\n",
       "        [-4.8553e-01],\n",
       "        [-1.3565e-01],\n",
       "        [-4.0577e-01],\n",
       "        [-5.6009e-02],\n",
       "        [ 4.3786e-04],\n",
       "        [ 2.3829e-01],\n",
       "        [ 2.2607e-01],\n",
       "        [ 1.0036e-01],\n",
       "        [ 1.8809e-01],\n",
       "        [-2.1362e-01],\n",
       "        [ 3.3259e-01],\n",
       "        [-1.2307e-01],\n",
       "        [ 4.9496e-03],\n",
       "        [ 4.2801e-01],\n",
       "        [-4.5748e-01],\n",
       "        [-4.9715e-01],\n",
       "        [ 3.3204e-01],\n",
       "        [ 3.8093e-02],\n",
       "        [-2.2992e-01],\n",
       "        [ 1.1048e-01],\n",
       "        [-2.5169e-01],\n",
       "        [-1.6640e-01],\n",
       "        [-3.4442e-01],\n",
       "        [-2.7108e-01],\n",
       "        [ 9.2912e-02],\n",
       "        [ 4.0776e-01],\n",
       "        [-3.5967e-01],\n",
       "        [ 3.3072e-01],\n",
       "        [ 1.1381e-01],\n",
       "        [ 1.1248e-01],\n",
       "        [ 6.9814e-02],\n",
       "        [ 3.5477e-01],\n",
       "        [-3.3067e-01],\n",
       "        [ 4.2600e-01],\n",
       "        [-3.3075e-01],\n",
       "        [-1.9542e-01],\n",
       "        [ 2.0494e-01],\n",
       "        [ 4.9375e-01],\n",
       "        [-9.4007e-02],\n",
       "        [-4.8039e-01],\n",
       "        [-4.0873e-02],\n",
       "        [-2.0670e-01],\n",
       "        [-2.9385e-01],\n",
       "        [ 1.7780e-01],\n",
       "        [-1.6597e-01],\n",
       "        [-4.1920e-01],\n",
       "        [ 4.6031e-01],\n",
       "        [-2.9383e-01],\n",
       "        [ 2.9449e-01],\n",
       "        [ 2.1703e-01],\n",
       "        [ 4.6106e-03],\n",
       "        [ 5.7114e-02],\n",
       "        [ 1.7683e-01],\n",
       "        [ 4.9137e-01],\n",
       "        [ 3.1946e-01],\n",
       "        [ 3.8538e-01],\n",
       "        [-9.7120e-02],\n",
       "        [-3.5136e-03],\n",
       "        [-1.2872e-01],\n",
       "        [ 3.3887e-01],\n",
       "        [ 1.0040e-01],\n",
       "        [ 2.9935e-01],\n",
       "        [ 4.0527e-01],\n",
       "        [ 8.1658e-02],\n",
       "        [ 3.5294e-03],\n",
       "        [-2.4669e-01],\n",
       "        [ 4.9724e-01],\n",
       "        [ 4.2291e-01],\n",
       "        [-4.1772e-01],\n",
       "        [-1.3623e-01],\n",
       "        [-1.0873e-01],\n",
       "        [ 4.5826e-01],\n",
       "        [-3.9279e-02],\n",
       "        [-4.2891e-01],\n",
       "        [ 3.2084e-01],\n",
       "        [ 1.9516e-01],\n",
       "        [ 1.2033e-01],\n",
       "        [-2.3979e-01],\n",
       "        [ 2.6284e-01],\n",
       "        [ 2.8072e-01],\n",
       "        [ 4.3747e-01],\n",
       "        [ 3.4341e-01],\n",
       "        [ 4.0925e-02],\n",
       "        [ 2.7566e-01],\n",
       "        [ 1.8229e-01],\n",
       "        [-3.7867e-01],\n",
       "        [-1.4606e-01],\n",
       "        [-4.8489e-01],\n",
       "        [ 3.4309e-01],\n",
       "        [-1.9924e-01],\n",
       "        [ 3.4779e-01],\n",
       "        [ 3.0401e-02],\n",
       "        [-2.5791e-01],\n",
       "        [ 3.1106e-01],\n",
       "        [-2.8766e-02],\n",
       "        [-2.1087e-01],\n",
       "        [-4.1718e-01],\n",
       "        [ 3.2302e-01],\n",
       "        [-2.7282e-01],\n",
       "        [ 2.6793e-01],\n",
       "        [-1.4590e-01],\n",
       "        [ 1.7498e-01],\n",
       "        [-1.8381e-01],\n",
       "        [ 1.5499e-01],\n",
       "        [ 2.0669e-01],\n",
       "        [-4.5512e-01],\n",
       "        [ 9.7337e-03],\n",
       "        [ 4.6790e-01],\n",
       "        [-8.1674e-02],\n",
       "        [ 4.6086e-02],\n",
       "        [-2.6864e-01],\n",
       "        [ 2.2739e-01],\n",
       "        [-3.9666e-01],\n",
       "        [ 4.3600e-01],\n",
       "        [ 3.8900e-01],\n",
       "        [ 4.6581e-01],\n",
       "        [-5.6697e-02],\n",
       "        [ 4.2238e-02],\n",
       "        [ 1.9612e-01],\n",
       "        [-7.8686e-03],\n",
       "        [ 3.1696e-01],\n",
       "        [ 1.4301e-01],\n",
       "        [ 2.4103e-01],\n",
       "        [ 4.1527e-01],\n",
       "        [-1.9633e-01],\n",
       "        [ 1.6831e-01],\n",
       "        [-2.6751e-01],\n",
       "        [-4.1596e-01],\n",
       "        [ 1.1301e-01],\n",
       "        [-2.2468e-01],\n",
       "        [ 4.1403e-01],\n",
       "        [ 2.5420e-01],\n",
       "        [-1.7919e-01],\n",
       "        [-2.3545e-01],\n",
       "        [ 4.5357e-02],\n",
       "        [-1.3032e-01],\n",
       "        [-1.6493e-01],\n",
       "        [ 1.8501e-01],\n",
       "        [ 1.3456e-01],\n",
       "        [ 4.9885e-02],\n",
       "        [ 4.6452e-01],\n",
       "        [ 2.2212e-01],\n",
       "        [ 1.4524e-01],\n",
       "        [ 2.0193e-01],\n",
       "        [ 4.2022e-01],\n",
       "        [ 2.5778e-01],\n",
       "        [-2.4952e-01],\n",
       "        [-2.4790e-01],\n",
       "        [ 3.9542e-01],\n",
       "        [ 4.0193e-01],\n",
       "        [-3.5338e-02],\n",
       "        [-1.6478e-01],\n",
       "        [ 1.0468e-01],\n",
       "        [-1.6771e-01],\n",
       "        [-1.4317e-01],\n",
       "        [ 1.0420e-01],\n",
       "        [-4.6168e-02],\n",
       "        [-8.7629e-02],\n",
       "        [-4.6943e-01],\n",
       "        [-4.6368e-01],\n",
       "        [-4.8790e-01],\n",
       "        [-4.7825e-01],\n",
       "        [-8.2655e-02],\n",
       "        [ 3.7725e-01],\n",
       "        [-3.3057e-02],\n",
       "        [-6.0694e-02],\n",
       "        [-6.0406e-02],\n",
       "        [-2.0683e-01],\n",
       "        [ 3.1352e-01],\n",
       "        [-5.4083e-02],\n",
       "        [-3.1323e-01],\n",
       "        [ 7.0943e-02],\n",
       "        [-4.1526e-01],\n",
       "        [-2.3065e-02],\n",
       "        [ 3.2835e-01],\n",
       "        [-2.2808e-01],\n",
       "        [ 4.8627e-01],\n",
       "        [-2.2353e-01],\n",
       "        [ 1.3338e-01],\n",
       "        [-4.7713e-01],\n",
       "        [-2.0347e-01],\n",
       "        [ 4.9078e-01],\n",
       "        [ 1.6311e-01],\n",
       "        [ 1.3928e-01],\n",
       "        [-3.8266e-01],\n",
       "        [-4.8915e-01],\n",
       "        [-2.5580e-01],\n",
       "        [-4.5159e-02],\n",
       "        [ 4.8851e-01],\n",
       "        [-2.5310e-01],\n",
       "        [ 2.9929e-01],\n",
       "        [-2.5044e-02],\n",
       "        [-2.5928e-01],\n",
       "        [ 8.8468e-02],\n",
       "        [-2.1830e-01],\n",
       "        [-4.1527e-02],\n",
       "        [ 1.3105e-01],\n",
       "        [-2.2585e-01],\n",
       "        [ 4.9126e-01],\n",
       "        [ 1.0675e-02],\n",
       "        [-4.5010e-01],\n",
       "        [-2.4114e-01],\n",
       "        [-2.8338e-01],\n",
       "        [ 3.0376e-01],\n",
       "        [ 9.7281e-02],\n",
       "        [-4.7535e-01],\n",
       "        [-1.3232e-01],\n",
       "        [-1.1023e-02],\n",
       "        [-2.7375e-02],\n",
       "        [ 1.3275e-02],\n",
       "        [ 1.4605e-02],\n",
       "        [ 4.2756e-01],\n",
       "        [-2.1847e-01],\n",
       "        [-4.9624e-01],\n",
       "        [-3.8236e-01],\n",
       "        [ 3.8851e-01],\n",
       "        [-4.7624e-01],\n",
       "        [-2.2542e-01],\n",
       "        [ 2.4047e-02],\n",
       "        [-2.1520e-03],\n",
       "        [ 1.2604e-01],\n",
       "        [-3.2319e-01],\n",
       "        [-3.7729e-01],\n",
       "        [-4.5415e-01],\n",
       "        [ 2.4399e-01],\n",
       "        [ 1.4445e-01],\n",
       "        [-4.0231e-01],\n",
       "        [ 5.2863e-02],\n",
       "        [-3.9726e-02],\n",
       "        [-4.3062e-01],\n",
       "        [ 1.5088e-01],\n",
       "        [-1.9606e-01],\n",
       "        [-1.2994e-01],\n",
       "        [ 9.8437e-02],\n",
       "        [ 4.1772e-01],\n",
       "        [-2.9061e-01],\n",
       "        [ 4.6627e-02],\n",
       "        [-1.1116e-01],\n",
       "        [ 2.3214e-01],\n",
       "        [ 2.9813e-01],\n",
       "        [-3.0601e-01],\n",
       "        [ 1.7815e-01],\n",
       "        [-2.9238e-01],\n",
       "        [-4.7222e-01],\n",
       "        [-1.0402e-02],\n",
       "        [ 4.7563e-01],\n",
       "        [ 2.8453e-01],\n",
       "        [-2.4313e-01],\n",
       "        [-9.1892e-04],\n",
       "        [ 2.5884e-01],\n",
       "        [-8.3088e-02],\n",
       "        [ 1.7379e-01],\n",
       "        [ 1.6198e-01],\n",
       "        [-3.8929e-01],\n",
       "        [ 1.8740e-01],\n",
       "        [-4.3458e-01],\n",
       "        [ 2.7277e-01],\n",
       "        [ 4.3556e-01],\n",
       "        [-3.2214e-01],\n",
       "        [ 1.6505e-01],\n",
       "        [-3.5144e-01],\n",
       "        [-4.8972e-01],\n",
       "        [ 4.9259e-01],\n",
       "        [-3.8809e-01],\n",
       "        [-1.4050e-01],\n",
       "        [ 8.2744e-02],\n",
       "        [ 4.1282e-02],\n",
       "        [ 3.7961e-01],\n",
       "        [-9.8022e-02],\n",
       "        [ 1.3539e-02],\n",
       "        [ 4.7495e-01],\n",
       "        [ 3.6832e-01],\n",
       "        [ 2.5383e-01],\n",
       "        [-2.1531e-01],\n",
       "        [ 2.9152e-01],\n",
       "        [-4.9912e-02],\n",
       "        [-1.7065e-01],\n",
       "        [-3.7530e-02],\n",
       "        [ 7.9540e-02],\n",
       "        [ 8.0982e-02],\n",
       "        [ 9.1386e-04],\n",
       "        [ 4.5251e-01],\n",
       "        [-1.4632e-01],\n",
       "        [ 8.7412e-02],\n",
       "        [-3.8000e-01],\n",
       "        [-1.8990e-01],\n",
       "        [-6.8142e-02],\n",
       "        [ 2.8262e-01],\n",
       "        [-2.4964e-01],\n",
       "        [-8.7172e-02],\n",
       "        [ 4.0573e-01],\n",
       "        [-2.5437e-01],\n",
       "        [ 4.4277e-01],\n",
       "        [ 2.9526e-01],\n",
       "        [-1.1693e-01],\n",
       "        [ 4.6266e-01],\n",
       "        [ 2.2428e-01],\n",
       "        [ 1.4086e-01],\n",
       "        [-2.9494e-01],\n",
       "        [ 1.6577e-01],\n",
       "        [ 4.1748e-01],\n",
       "        [ 2.7076e-01],\n",
       "        [ 3.3338e-01],\n",
       "        [ 2.6954e-02],\n",
       "        [-6.1104e-02],\n",
       "        [ 1.4456e-01],\n",
       "        [ 4.4525e-01],\n",
       "        [-4.6001e-01],\n",
       "        [ 3.3221e-01],\n",
       "        [-4.6114e-01],\n",
       "        [-7.6808e-02],\n",
       "        [-2.2634e-01],\n",
       "        [-9.1575e-03],\n",
       "        [ 8.3545e-02],\n",
       "        [-1.0921e-01],\n",
       "        [ 2.7601e-01],\n",
       "        [ 1.6550e-01],\n",
       "        [ 3.0898e-01],\n",
       "        [-1.9059e-01],\n",
       "        [-2.1527e-01],\n",
       "        [ 4.2469e-01],\n",
       "        [ 2.7134e-01],\n",
       "        [-1.9609e-02],\n",
       "        [-3.5532e-02],\n",
       "        [ 4.6181e-01],\n",
       "        [ 4.1779e-01],\n",
       "        [ 1.4599e-01],\n",
       "        [-4.7318e-01],\n",
       "        [ 3.1552e-01],\n",
       "        [ 1.9336e-01],\n",
       "        [-5.7743e-02],\n",
       "        [ 1.1880e-01],\n",
       "        [-3.5838e-01],\n",
       "        [ 1.5951e-01],\n",
       "        [-3.0083e-01],\n",
       "        [ 1.5424e-01],\n",
       "        [ 1.0150e-01],\n",
       "        [-3.0523e-01],\n",
       "        [-4.6945e-01],\n",
       "        [-1.9691e-02],\n",
       "        [ 1.4350e-01],\n",
       "        [-1.5218e-01],\n",
       "        [-2.7879e-01],\n",
       "        [ 4.8880e-01],\n",
       "        [ 9.2471e-02],\n",
       "        [-1.7095e-01],\n",
       "        [ 3.0645e-01],\n",
       "        [ 1.3440e-01],\n",
       "        [-4.1332e-01],\n",
       "        [-3.3806e-02],\n",
       "        [ 7.4592e-02],\n",
       "        [ 6.0539e-02],\n",
       "        [-4.8018e-01],\n",
       "        [ 4.5297e-01],\n",
       "        [-2.2942e-01],\n",
       "        [-3.9124e-01],\n",
       "        [ 2.2235e-02],\n",
       "        [ 1.0056e-01],\n",
       "        [ 3.7892e-01],\n",
       "        [ 4.5109e-01],\n",
       "        [-5.9058e-02],\n",
       "        [ 4.2191e-01],\n",
       "        [ 1.4371e-02],\n",
       "        [ 3.1669e-01],\n",
       "        [ 3.8689e-01],\n",
       "        [-2.4892e-01],\n",
       "        [ 1.9752e-01],\n",
       "        [-2.4725e-01],\n",
       "        [ 1.8013e-01],\n",
       "        [ 1.4405e-01],\n",
       "        [-2.6369e-02],\n",
       "        [ 1.7561e-01],\n",
       "        [-4.7406e-01],\n",
       "        [-3.4693e-01],\n",
       "        [ 3.7549e-01],\n",
       "        [ 3.9003e-01],\n",
       "        [ 3.3879e-02],\n",
       "        [-1.7336e-02],\n",
       "        [ 3.9236e-01],\n",
       "        [-2.4711e-01],\n",
       "        [ 3.0122e-01],\n",
       "        [-1.4382e-01],\n",
       "        [ 1.9877e-02],\n",
       "        [ 2.3752e-01],\n",
       "        [-3.3693e-01],\n",
       "        [-3.8771e-01],\n",
       "        [-2.4682e-01],\n",
       "        [-1.8840e-01],\n",
       "        [-1.1388e-01],\n",
       "        [ 3.9059e-01],\n",
       "        [ 3.8441e-01],\n",
       "        [-4.8068e-01],\n",
       "        [ 4.8008e-01],\n",
       "        [ 2.9492e-01],\n",
       "        [ 2.3943e-02],\n",
       "        [ 6.8429e-02],\n",
       "        [ 2.2401e-01],\n",
       "        [ 1.8695e-01],\n",
       "        [ 1.5651e-02],\n",
       "        [-1.9616e-01],\n",
       "        [-4.2769e-01],\n",
       "        [-5.2367e-02],\n",
       "        [ 8.6477e-02],\n",
       "        [ 1.5936e-01],\n",
       "        [-2.7689e-01],\n",
       "        [ 2.2130e-01],\n",
       "        [ 7.2325e-03],\n",
       "        [-4.6553e-01],\n",
       "        [-4.8097e-01],\n",
       "        [ 2.1063e-01],\n",
       "        [ 2.5409e-01],\n",
       "        [-1.9597e-01],\n",
       "        [ 8.0330e-02],\n",
       "        [-1.0553e-02],\n",
       "        [-3.5091e-01],\n",
       "        [-1.5151e-01],\n",
       "        [ 3.6102e-01],\n",
       "        [-4.3016e-01],\n",
       "        [-2.8113e-01],\n",
       "        [-2.1028e-01],\n",
       "        [ 3.8773e-01],\n",
       "        [ 3.5760e-01],\n",
       "        [ 1.3183e-01],\n",
       "        [-2.7907e-02],\n",
       "        [ 2.2920e-01],\n",
       "        [ 1.0299e-01],\n",
       "        [ 2.6070e-02],\n",
       "        [ 4.0315e-01],\n",
       "        [ 9.5356e-02]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# torch.manual_seed(0)\n",
    "n_coeffs = X.shape[1]\n",
    "coeffs = torch.rand(n_coeffs, 1) - 0.5\n",
    "coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_pred(X, coeffs):\n",
    "    return torch.sigmoid(X @ coeffs)\n",
    "\n",
    "def calc_loss(pred, y):\n",
    "    return torch.abs(pred - y).mean()\n",
    "\n",
    "def init_coeffs():\n",
    "    return (torch.rand(n_coeffs, 1)*0.1).requires_grad_()\n",
    "\n",
    "def update_coeffs(coeffs, lr):\n",
    "    coeffs.sub_(lr * coeffs.grad)\n",
    "    coeffs.grad.zero_()\n",
    "\n",
    "def accuracy(coeffs, X, y):\n",
    "    preds = calc_pred(X, coeffs)\n",
    "    return (y.bool() == (preds > 0.5)).float().mean()\n",
    "\n",
    "def one_epoch(coeffs, lr, X, y):\n",
    "    loss = calc_loss(calc_pred(X, coeffs), y)\n",
    "    loss.backward()\n",
    "    with torch.no_grad(): update_coeffs(coeffs, lr)\n",
    "    print(f\"Loss: {loss:.4f}, Accuracy {accuracy(coeffs, X, y):.4f}\", end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(658, 164)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastai.data.transforms import RandomSplitter\n",
    "trn_split, val_split = RandomSplitter()(new_df) # can set seed\n",
    "X_train, X_val = X[trn_split], X[val_split]\n",
    "y_train, y_val = y[trn_split][:, None], y[val_split][:, None]\n",
    "len(y_train), len(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs=30, lr=0.1, X=X_train, y=y_train):\n",
    "    # torch.manual_seed(0)\n",
    "    coeffs = init_coeffs()\n",
    "    for i in range(epochs):\n",
    "        print(f\"Epoch {i+1}: \", end='')\n",
    "        one_epoch(coeffs, lr, X, y)\n",
    "        print()\n",
    "    return coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss: 0.4982, Accuracy 0.5350 \n",
      "Epoch 2: Loss: 0.4955, Accuracy 0.5365 \n",
      "Epoch 3: Loss: 0.4929, Accuracy 0.5410 \n",
      "Epoch 4: Loss: 0.4903, Accuracy 0.5456 \n",
      "Epoch 5: Loss: 0.4877, Accuracy 0.5547 \n",
      "Epoch 6: Loss: 0.4852, Accuracy 0.5714 \n",
      "Epoch 7: Loss: 0.4827, Accuracy 0.5866 \n",
      "Epoch 8: Loss: 0.4802, Accuracy 0.5988 \n",
      "Epoch 9: Loss: 0.4777, Accuracy 0.6064 \n",
      "Epoch 10: Loss: 0.4753, Accuracy 0.6094 \n",
      "Epoch 11: Loss: 0.4729, Accuracy 0.6170 \n",
      "Epoch 12: Loss: 0.4706, Accuracy 0.6216 \n",
      "Epoch 13: Loss: 0.4683, Accuracy 0.6216 \n",
      "Epoch 14: Loss: 0.4661, Accuracy 0.6216 \n",
      "Epoch 15: Loss: 0.4639, Accuracy 0.6246 \n",
      "Epoch 16: Loss: 0.4617, Accuracy 0.6292 \n",
      "Epoch 17: Loss: 0.4596, Accuracy 0.6337 \n",
      "Epoch 18: Loss: 0.4576, Accuracy 0.6353 \n",
      "Epoch 19: Loss: 0.4556, Accuracy 0.6429 \n",
      "Epoch 20: Loss: 0.4536, Accuracy 0.6444 \n",
      "Epoch 21: Loss: 0.4517, Accuracy 0.6444 \n",
      "Epoch 22: Loss: 0.4498, Accuracy 0.6489 \n",
      "Epoch 23: Loss: 0.4480, Accuracy 0.6520 \n",
      "Epoch 24: Loss: 0.4462, Accuracy 0.6550 \n",
      "Epoch 25: Loss: 0.4444, Accuracy 0.6641 \n",
      "Epoch 26: Loss: 0.4427, Accuracy 0.6687 \n",
      "Epoch 27: Loss: 0.4410, Accuracy 0.6702 \n",
      "Epoch 28: Loss: 0.4394, Accuracy 0.6733 \n",
      "Epoch 29: Loss: 0.4377, Accuracy 0.6748 \n",
      "Epoch 30: Loss: 0.4362, Accuracy 0.6763 \n"
     ]
    }
   ],
   "source": [
    "coeffs = train_model(lr=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_coeffs():\n",
    "    return dict(zip(vectorizer.get_feature_names_out(), coeffs.requires_grad_(False).numpy().flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fun</td>\n",
       "      <td>0.946524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>funny</td>\n",
       "      <td>0.867420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>engaging</td>\n",
       "      <td>0.803172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>interesting</td>\n",
       "      <td>0.553711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>warm</td>\n",
       "      <td>0.450060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>going</td>\n",
       "      <td>-0.335502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>easy</td>\n",
       "      <td>-0.349811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>talkative</td>\n",
       "      <td>-0.491349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>friendly</td>\n",
       "      <td>-0.605620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>polite</td>\n",
       "      <td>-0.831689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>597 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            word    weight\n",
       "0            fun  0.946524\n",
       "1          funny  0.867420\n",
       "2       engaging  0.803172\n",
       "3    interesting  0.553711\n",
       "4           warm  0.450060\n",
       "..           ...       ...\n",
       "592        going -0.335502\n",
       "593         easy -0.349811\n",
       "594    talkative -0.491349\n",
       "595     friendly -0.605620\n",
       "596       polite -0.831689\n",
       "\n",
       "[597 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_df = pd.DataFrame.from_dict(show_coeffs(), orient='index', columns=['weight'])\n",
    "weights_df.sort_values('weight', ascending=False, inplace=True)\n",
    "weights_df.reset_index(inplace=True)\n",
    "weights_df.rename(columns={'index': 'word'}, inplace=True)\n",
    "weights_df.to_excel('weights.xlsx', index=False)\n",
    "weights_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6524)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_preds = calc_pred(X_val, coeffs)\n",
    "results = y_val.bool() == (val_preds > 0.5)\n",
    "results.float().mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import *\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b116f20c23c42848ad36c3e44a32791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/822 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = Dataset.from_pandas(new_df)\n",
    "ds = ds.rename_column('bool', 'labels')\n",
    "ds = ds.cast_column('labels', Value('float32'))\n",
    "dds = ds.train_test_split(seed=2005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "d:\\Users\\selva\\Documents\\Coding\\GuardianBlindDate\\venv\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'microsoft/deberta-v3-small'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "151127516f6245c59085d6fab0e49061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/616 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f98e7741a124052b9c95c6d49e79fa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/206 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_func(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding=True)\n",
    "dds = dds.map(tokenize_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "metric = evaluate.load('accuracy')\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.round(logits)\n",
    "    return metric.compute(predictions=preds, references=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2ForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "bs = 16\n",
    "lr = 1e-5\n",
    "epochs = 16\n",
    "# args = TrainingArguments('outputs', learning_rate=lr, per_device_train_batch_size=bs, per_device_eval_batch_size=bs, num_train_epochs=epochs,\n",
    "#                          evaluation_strategy='epoch', save_strategy='epoch', weight_decay=0.01, warmup_ratio=0.1, load_best_model_at_end=True, metric_for_best_model='accuracy'\n",
    "#                         ) # auto_find_batch_size? (requires pip install accelerate)\n",
    "\n",
    "args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=False,\n",
    "    evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n",
    "    num_train_epochs=epochs, weight_decay=0.01, report_to='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dds['train'],\n",
    "    eval_dataset=dds['test'],\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: score, text. If score, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "d:\\Users\\selva\\Documents\\Coding\\GuardianBlindDate\\venv\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 616\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 156\n",
      "  Number of trainable parameters = 141895681\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8102684df5c54e3a9504c9a3626ffd34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/156 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: score, text. If score, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 206\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "656a0c508fb1498aade1a6985c2b9b8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.27644243836402893, 'eval_accuracy': 0.6456310679611651, 'eval_runtime': 4.06, 'eval_samples_per_second': 50.739, 'eval_steps_per_second': 1.724, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: score, text. If score, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 206\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "165f2c0d878d4ac88891f6ce63f0fc3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2939416468143463, 'eval_accuracy': 0.6359223300970874, 'eval_runtime': 3.251, 'eval_samples_per_second': 63.365, 'eval_steps_per_second': 2.153, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: score, text. If score, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 206\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85a7bcf8092b4c56888758b88261b0f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2984012961387634, 'eval_accuracy': 0.6456310679611651, 'eval_runtime': 3.134, 'eval_samples_per_second': 65.731, 'eval_steps_per_second': 2.234, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: score, text. If score, text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 206\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e84ca3fe60dc4d6dacfe611dc02e6c46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3007143437862396, 'eval_accuracy': 0.6407766990291263, 'eval_runtime': 4.166, 'eval_samples_per_second': 49.448, 'eval_steps_per_second': 1.68, 'epoch': 4.0}\n",
      "{'train_runtime': 315.2646, 'train_samples_per_second': 7.816, 'train_steps_per_second': 0.495, 'train_loss': 0.07165377567975949, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=156, training_loss=0.07165377567975949, metrics={'train_runtime': 315.2646, 'train_samples_per_second': 7.816, 'train_steps_per_second': 0.495, 'train_loss': 0.07165377567975949, 'epoch': 4.0})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\selva/.cache\\huggingface\\hub\\models--microsoft--deberta-v3-small\\snapshots\\a36c739020e01763fe789b4b85e2df55d6180012\\config.json\n",
      "Model config DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-small\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "loading file spm.model from cache at C:\\Users\\selva/.cache\\huggingface\\hub\\models--microsoft--deberta-v3-small\\snapshots\\a36c739020e01763fe789b4b85e2df55d6180012\\spm.model\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\selva/.cache\\huggingface\\hub\\models--microsoft--deberta-v3-small\\snapshots\\a36c739020e01763fe789b4b85e2df55d6180012\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\selva/.cache\\huggingface\\hub\\models--microsoft--deberta-v3-small\\snapshots\\a36c739020e01763fe789b4b85e2df55d6180012\\config.json\n",
      "Model config DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-small\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Adding [MASK] to the vocabulary\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "loading configuration file config.json from cache at C:\\Users\\selva/.cache\\huggingface\\hub\\models--microsoft--deberta-v3-small\\snapshots\\a36c739020e01763fe789b4b85e2df55d6180012\\config.json\n",
      "Model config DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-small\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "d:\\Users\\selva\\Documents\\Coding\\GuardianBlindDate\\venv\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-v3-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.7054358124732971}]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer) # you can infer the task if the model hasn't been fine-tuned, I think\n",
    "\n",
    "text = \"interesting\"\n",
    "classifier(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engaging, interesting, cool. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Charming, hard-working, good guy. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Witty, smart, hard-working. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Funny, chatty, polite. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      " Bubbly, pretty, outdoorsy. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Attentive, kind, interesting. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Funky roller chick. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Up for it (a fun and free night). [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Delightful, insomniac, creative. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Sensitive, adventurous, on the brink of something new? [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Happy, interesting, energetic. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Three probably isn’t fair. Grounded, curious, joyful. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "A little unabashed. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Friendly, insightful, sensitive. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Creative, outgoing, personable. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Friendly, polite and warm. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Fun, caring, driven. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Stylish, composed, confident. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Kind, understanding, driven. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Handsome, affable, genuine. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Kind, intelligent, great company (I cheated and used four). [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Open, authentic, creative. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Convivial, successful, erudite. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Fiery, seductive, sensitive. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Curious, open-minded, fun. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Fun, chatty, foodie. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Scholarly, eloquent, ambitious. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Funny, smart, engaging. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Charming, funny, interesting. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Outgoing, intelligent, unpretentious. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Sweet, attentive and well mannered. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Charismatic, handsome, friendly. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Smart, upbeat, focused. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Articulate, friendly, chatty. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Genuine, passionate, sophisticated. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Smart, interesting, independent. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Brainy, chirpy, friendly. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Attentive, ambitious, accomplished. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Chatty, energetic, decisive. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Friendly, funny, well-read. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Creative, warm, ambitious. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Engaging, witty, relaxed. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Intriguing, fun, charming. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Confident, kind, funny. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Quite attractive (blurry screen), funny and a good conversationalist. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Towering Brixton hipster. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Honest, lively, attractive. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Attractive, intelligent, confident. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "A true gent – he walked me to the bus stop. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Thoroughly lovely chap. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Fun, engaging, open. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Smiley, interesting, polite. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Smiley, confident, fun. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Not for me. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Attentive, forthcoming, charming. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Fun, easy-going, down-to-earth. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Talented, funny, intelligent. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Genuine, warm, determined. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Fun, social and engaging. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Easygoing, funny, smart. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      " Happy, funny, catman. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Interesting, dashing, chatty. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Chatty, engaging, open. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Chatty, inquisitive, engaging. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Cute, kind, erudite. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Nice, funny and, er, vegetarian. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      " Confident, flamboyant, nice. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Sophisticated, smiley, chatty. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Young, well-educated and passionate. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Intelligent, kind, funny. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Friendly, conservative and kind. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Talented, interesting, knowledgeable. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Gentlemanly, fun, interesting. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Confident, calm, entertaining. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Kind, funny, enthusiastic. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Driven, intelligent, charming. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Intelligent, confident, refreshing. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Eccentric, talkative, intelligent. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Attractive, friendly, funny. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Intelligent, polite, quiet. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Intelligent, warm, witty. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Funny, intelligent, intriguing. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Witty, tactile, original. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Genuine, kind, sparkly. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Calm, sweet, caring. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Chatty, easy-going, amusing. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Smart, considerate, relaxed. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Fun, friendly company. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Interesting, creative, relaxed. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Warm, responsive, easygoing. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Inviting, pure and creative. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Affable, well-mannered, personable. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Anti-cake pickler. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Confident, fashionable, grounded. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Pleasant, cerebral, unassuming. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Intellectual, relaxed, enthusiastic. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Interesting, affectionate, kind-hearted. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Determined, organised, open. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Insightful, thoughtful, empathic. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Fun, loyal and... showbiz. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "A decent guy. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Relaxed, cheeky, intelligent. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Eloquent, witty, driven. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Courteous, kind, sanguine. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Nice, English, chappy. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Pretty, sociable, grafter. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Greek, stylish, fun. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Insightful, fun, courageous. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Better than expected. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Friendly, fun, reflective. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Funny, enthusiastic, talkative. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Calm, confident, cultured. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Sexy, funny and smart. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Engaging, optimistic, clever. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      " Funny, game-for-a-laugh, gin-loving. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Vibrant. Chatty. Optimistic. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "London, funny, easy-to-talk-to. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Intelligent, kind, shy. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Definitely not shy. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Should research oranges. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Friendly, bright, grounded. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Youthful, resilient, intelligent. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Relaxed, engaging and attractive. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Not a Trekkie. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Ambitious, outgoing, cool. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Open, amiable, fun. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "A good laugh. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Interesting. Fun. Gin. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Easy-going, friendly, gentlemanly. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Dalston’s hottest ticket. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "A good egg. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Nervous, fit, talented. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Friendly, affable gentleman. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Fun and entertaining. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Elegant, thoughtful, curious. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Warm, intelligent, enthusiastic. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Kind, friendly and interesting. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      " Happy, private, passionate. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Not my type. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Friendly, stylish, funny. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Warm, genuine, self-assured. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Pleasant, honest, kind. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Thoughtful, inquisitive, very good hair. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Warm, free-spirited, creative. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Confident, jolly and polite. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Confident, ambitious, intelligent. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Funny, friendly, engaging. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Intelligent, chilled, confident. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Interesting, humble, gorgeous. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Artistic, intelligent, lovely. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Intelligent, articulate, easy-going. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Vibrant, amiable, easy-going. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Tall, blond, smiley. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Funny, cute, friendly. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Gracious, passionate, empathetic. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Bubbly, outgoing, youthful. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Easygoing, positive, ambitious. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Engaging, fun, interesting. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Laid-back, funny, dairy-free. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Hilarious, loquacious, effervescent. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Smiley, interesting, arty. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Honest, funny and easygoing. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Interesting, chatty and intelligent. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Upbeat, funny, friendly. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Interested, pleasant, easy-going. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Smartly dressed, fun, chatty. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      " Talkative, enthusiastic, friendly. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Ambitious, chilled and a traveller. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "On a journey. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      " Erudite, stunning, adventurous. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Chatty, witty, chilled-out. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      " Fun, interesting, inquisitive. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Kind, interesting and honest. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Fun, open, funny. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "A lovely person. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Salt of (the) earth. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Chatty, musical, laid-back. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Fun, warm, raconteur. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Friendly, interesting, laid-back. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Affectionate, sarcastic, seductive. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Charismatic, cheeky, wise. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Sweet, intelligent, chill. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Looking for herself. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      " Chatty, creative, curious. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Likes a drink. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Smart, funny, genuine. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Animated, principled, green. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Confident, adventurous, fun-loving. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Articulate, well-mannered, thoughtful. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Empathetic, ambitious, collected. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Bright, engaging, effervescent. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Intelligent, kind, fun. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Charming, funny, interesting. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Talkative, outgoing and fun. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Just my type. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Social, sporty and curious. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "A natural blond. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Friendly, genuine, mackem. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Warm, interesting, sophisticated. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Knows his decking. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Chatty, funny, creative. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      "Thoughtful. Attentive. Calm. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Well-read, interesting, tall. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "A good guy. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 0.0\n",
      "Conscientious, friendly, relaxed. Those don’t do her justice, though.  [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n",
      " Adventurous, creative, fun and – I'm being greedy with a fourth – gorgeous. [{'label': 'LABEL_0', 'score': 0.7054358124732971}] 1.0\n"
     ]
    }
   ],
   "source": [
    "for example in dds['test']:\n",
    "    print(example['text'], classifier(text), example['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                             Good-humoured, creative, curious.\n",
       "1                                                     Bubbly, fun, intelligent.\n",
       "2                                             Enthusiastic, energetic, musical.\n",
       "3                                                       Smart, American, sweet.\n",
       "4      We discussed this during the date: energetic (not chaotic), fun, honest.\n",
       "                                         ...                                   \n",
       "817                                 Bright, bubbly and bonkers (in a good way).\n",
       "818                                             Erudite, stunning, adventurous.\n",
       "819                                                   Kind, funny, intelligent.\n",
       "820                                                Polite, engaging, carnivore.\n",
       "821                                                         Lively, happy, fun.\n",
       "Name: text, Length: 822, dtype: object"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   1, 3270,  261, 1257,  261,  785,    2]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"funny, interesting, fun\", return_tensors=\"pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d267c39223eb2a444f7b51aaabb83d648413f5a98be4d3157e2d2b8f4fd61ed3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
